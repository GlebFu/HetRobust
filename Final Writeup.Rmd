---
title: 'Replication: A df Approximation for a t-statistic with heterogeneous variance'
author: "Gleb Furman"
date: "May 14, 2015"
output: html_document
---

###Introduction
Lipsitz and Ibrahim (1999) sought to investigate the robustness of ordinary least squares (OLS) linear regression when the assumption of homoscedasticity is violated. This assumption is difficult to test when sample sizes are small and OLS parameter estimates, if left unadjusted, are biased and inconsistent. Although transforming the response and covariates can result in a regression model with constant variance, transformations often result in results that are difficult to interpret. Jackknife methods and estimated generalized least squares (EGLS) estimators are also often used in such cases to estimate variance and bias, however even when variance is modeled correctly, small sample sizes can lead to unstable estimates and low confidence interval coverage rates. The authors therefore propose the use of ordinary least squares estimation with a robust variance as well as a degrees-of-freedom correction. The authors compared the results of the following four conditions:

1. OLS
 + $\hat{\sigma}_i^2 = \hat{\sigma}^2$
 + $df_p = n-p$
2. OLS
 + $\hat{\sigma}_i^2 = e_i^2/1-h_i$
 + $df_p = n-p$
3. OLS
 + $\hat{\sigma}_i^2 = e_i^2/1-h_i$
 + $df_p = \hat{f}_p$
4. EGLS
 + $\hat{\sigma}_i^2 = \hat{a}_0 + \hat{a}_1x_p$
 + $df_p = n-p$
 
 The current study seeks to replicate the first three conditions.

```{r, cache = T}
library(devtools)
library(sandwich)
library(psych)
library(stringr)
library(devtools)
library(plyr)
library(Pusto)
library(stargazer)
library(dplyr)
library(knitr)
library(reshape2)
library(htmlTable)
#install_github("jepusto/Pusto")
#install.packages("devtools")
#install.packages("knitr")
rm(list = ls())
```

###Methods

```{r, cache = T}

```

####Data-Generating Model
The data-generating function (`gen_lm`) returns a fitted model given three specifications: sample size (`m`, integer), heteroscedasticity (`ht`, boolean), and a vector of 3 $\beta$ coefficients (`B`, integer). The responses are fixed (1,1.5,2,2.5,3,3.5,4,5,6,7,8,10), therefore `m` must be a multiple of 12. If `ht` is false, the variance of the error (`e_var`) is set to 1. If true, `e_var` is set to the response string, such that $\epsilon_i$ ~ _N_(0,$x_i$). Error values (`E`) are randomly selected from a normal distribution with standard deviation equal to sqrt(`e_var`). Responses (`y`) are then generated with the following formula: $y = \beta_0 + \beta_1x + \beta_2x^2 + e$.

```{r, cache = T, results = "asis"}
gen_lm <- function(m, ht, B) {
  x <- rep(c(1,1.5,2,2.5,3,3.5,4,5,6,7,8,10), m/12)
  e_var = 1
  if(ht) e_var <- x
    
  E <- rnorm(length(x),0,sqrt(e_var))
  y <- B[1] + B[2]*x + B[3]*x^2 + E
  
  return(lm(y ~ x + I(x^2)))
}

#Demo
test_lm <- gen_lm(12, T, c(0, .4, -.25))
stargazer(test_lm, type = "html", single.row=TRUE, title = "Table 1: Sample Model Generated by gen_lm")
stargazer(model.matrix(test_lm), type = "html", single.row=TRUE, digits = 1, title = "Table 2: Sample Model Matrix of gen_lm Output")
```

####Estimation Procedures
The estimation procedures consist of three functions: `A_calc`, `f_p`, and `estimate`. `A_calc` uses the Satterthwaite approximation to estimate the degrees-of-freedom for a given coefficient, and returns the estimated df. It takes the following list of arguments:

* `X` - A model matrix
* `H` - A hat matrix
* `h` - A diagonal of the hat matrix
* `W` - A vector of $1/sqrt(1-h)$
* `e` - A vector of model residuals
* `I` - An $n*n$ identity matrix 
* `p` - An indicator of which coefficient degrees-of-freedom are being estimated for

`f_p` takes as an argument a model generated by `gen_lm` and calculates the above values for the model. It then passes them through to `A_calc` for each of the `p` coefficients, and returns a vector of estimated degrees-of-freedom.

```{r, cache = T}
A_calc <- function(X, W, e, h, H, I, p) {
  c <- (solve(t(X) %*% X) %*% t(X))[p,]
  A <- diag((W * c)^2)
    
  var_B <- t(e) %*% A %*%e
  
  o4 <- e^4 / (3 * (1-h)^2)
  o2o2 <- e^2 %*% t(e)^2 / (2*H^2 + (1-h) %*%t (1-h))
  diag(o2o2) <- o4
  sigma_hat <- o2o2
  B <- ((I-H) %*% A %*% (I-H))
  
  num <- ((var_B)^2)
  den <- sum(as.vector(t(B)) * as.vector(B * sigma_hat))
  
  return(num/den)
}

f_p <- function(model) {
  X <- model.matrix(model)
  H <- X %*% solve(t(X) %*% X) %*% t(X)
  h <- diag(H)
  I <- diag(1, nobs(model))
  W <- 1 / sqrt(1 - h)
  e <- model$residuals

  return(c(A_calc(X, W, e, h, H, I, 1),
           A_calc(X, W, e, h, H, I, 2),
           A_calc(X, W, e, h, H, I, 3)))
}

#Demo
f_p(test_lm)
```

Finally, `estimate` is the main estimation driver function. It takes four arguments: the `gen_lm` generated linear model (`model`), a variance switch (`var`, string), a vector of 3 $\beta$ coefficients (`B`, integer), and a degrees-of-freedom switch (`df`, string). It then calculates the following list of variables:

* `sd_e` - Vector of standard errors for each $\beta$
* `HC2`  - Vector of standard errors for each $\beta$ using robust variance estimation
* `coef` - Vector of model coefficients
* `n`    - Number of observations for each $\beta$
* `p`    - Number of coefficients for each $\beta$
* `df`   - degrees-of-freedom switch evaluated as `n` - `p` or `f_p(model)` for each $\beta$
* `t`    - t-statistic cut off given `df` degrees-of-freedom for each $\beta$
* `ci`   - Confidence interval calculated with standard error switch evaluated as `sd_e` or `HC2`

The function then returns the following sets of values concatenated:

* A vector of 1's and 0's indicating whether a coefficient was captured by the confidence interval
* A vector of degrees-of-freedom calculated for each coefficient
* a vector of confidence interval spans for each confidence interval

```{r, cache = T, results = "asis"}
estimate <- function(model, var, B, df) {
  require(sandwich)
  
  sd_e <- summary(model)$coefficients[,"Std. Error"]
  HC2 <- sqrt(diag(vcovHC(model, type = "HC2")))
  coef <- summary(model)$coefficients[,"Estimate"]
  n <- rep(nobs(model), length(B))
  p <- rep(length(model$coef), length(B))
  df <- eval(parse(text = df))
  t <- qt(.975, df)
  ci <- rbind(coef,coef) + matrix(c(-1,1)) %*% (t*eval(parse(text = var)))
  return(c((B > ci[1,]) * (B < ci[2,]),(ci[2,]-ci[1,])/2,df))
}

#Demo
testimate<- rbind(estimate(test_lm, "sd_e", c(0,.4,-.25), "n-p"),
                  estimate(test_lm, "HC2", c(0,.4,-.25), "n-p"),
                  estimate(test_lm, "sd_e", c(0,.4,-.25), "f_p(model)"),
                  estimate(test_lm, "HC2", c(0,.4,-.25), "f_p(model)"))
colnames(testimate) <- c("B0_Cov&nbsp;&nbsp;", 
                      "B1_Cov&nbsp;&nbsp;", 
                      "B2_Cov&nbsp;&nbsp;",
                      "B0_Len&nbsp;&nbsp;",
                      "B1_Len&nbsp;&nbsp;",
                      "B2_Len&nbsp;&nbsp;",
                      "B0_df&nbsp;&nbsp;",
                      "B1_df&nbsp;&nbsp;",
                      "B2_df&nbsp;&nbsp;")

stargazer(round(testimate, 2), type = "html", title = "Table 3: Sample estimate() Outputs")
```

####Performance Criteria
The `performance` function takes the mean of all of the simulated values outputted by `estimate` (coverage rate, estimated degrees-of-freedom, and confidence interval length), renames each column appropriately, and returns a vector of the means.
```{r, cache = T}
performance <- function(results) {
  means <- rowMeans(results)
  names(means) <- c("B0_Cov", 
                      "B1_Cov", 
                      "B2_Cov",
                      "B0_Len",
                      "B1_Len",
                      "B2_Len",
                      "B0_df",
                      "B1_df",
                      "B2_df")
  
  return(means)
}
```

####Simulation Driver
The simulation drive, `runSim`, takes 8 design factors:

* `iterations` - Number of iterations to be simulated
* `m`   - Sample size (integer)
* `ht`  - Heteroscedastity (boolean)
* `B0`  - First coefficient (integer)
* `B1`  - Second coefficient (integer)
* `B2`  - Third coefficient (integer)
* `var` - Error variance estimator (character)
* `df`  - degrees-of-freedom indicator (character)
* `seed`- Seed, random if not passed (integer)

Based on the number of iterations, a replication function is called which generates a model using `gen_lm`, passing through the sample size, whether or not heteroscedasticity should be included, and a vector of the coefficients. The model is then passed through to `estimate` along with the error variance indicator, a vector of the coefficients, and the degrees-of-freedom indicator. The results of the `replicate` function are then passed through to the `performance` function. The results of the performance function are transposed, converted to a data frame, and passed back from the simulation driver.

```{r, cache = T, results = "asis"}
runSim <- function(iterations, m, ht, B0, B1, B2, var, df, seed = NULL) {
  if (!is.null(seed)) set.seed(seed)
  
  results <- replicate(iterations, {
    model <- gen_lm(m, ht, c(B0, B1, B2))
    estimate(model, var, c(B0, B1, B2), df)
  })
  
  perf <- as.data.frame(t(performance(results)))
  return(perf)
}

#Demo
testrun <- runSim(iterations = 1825,
                   m = 12,
                   ht = T,
                   B0 = 0,
                   B1 = .4,
                   B2 = -.25,
                   var = "HC2",
                   df = "n-p")

kable(round(testrun,3), caption = "__Table 4: Sample runSim() Output__")


```


####Experimental Design
Lipsitz and Ibrahim were interested in three outcome variables under conditions of heterogeneous and homogeneous variance (`ht`): confidence interval coverage rate, estimated degrees-of-freedom, and confidence interval length. These outcomes were investigating by manipulating the estimation of the error variance (`var`) and the degrees-of-freedom (`df`) used in calculating the t-statistic. Sample size was also manipulated (`m`). The authors called for a fixed set of 12 responses which are included in the `gen_lm` function, as well as 3 coefficients with a fixed set of values (`B0`, `B1`, `B2`). 1825 `iterations` were run in the original study under each condition. For the current study, a new random seed was generated for each condition.
```{r, cache = T}
set.seed(20150430)

design_factors <- list(m = c(12,24,48), 
                       ht = c(T,F), 
                       B0 = 0, 
                       B1 = .4, 
                       B2 = -.25, 
                       var = c("sd_e", "HC2"), 
                       df = c("n-p","f_p(model)"))
params <- expand.grid(design_factors, stringsAsFactors = F)
params$iterations <- 1825
params$seed <- round(runif(nrow(params)) * 2^30)

#Demo
sapply(design_factors, length)
nrow(params)
head(params)
```

###Results
```{r, cache = T, results = "asis"}
source_obj <- ls()
cluster <- start_parallel(source_obj)

system.time(results <- mdply(params, .fun = runSim, .parallel = TRUE))

stopCluster(cluster)

```

```{r, cache = T}

estimates <- select(results, m, ht, var, df, B0_Cov:B2_df)[-(13:18),]
estimates[,5:7] <- round(estimates[,5:7],3)
estimates[,8:13] <- round(estimates[,8:13],2)
estimates[,5:7] <- estimates[,5:7]*100
estimates$condition <- paste(estimates$var, estimates$df)

paper <- read.csv("Paper Results.csv")[-(13:18),]
paper$condition <- paste(paper$var, estimates$df)

difference <- estimates
difference[,5:13] <- round(select(estimates, B0_Cov:B2_df) - select(paper, B0_Cov:B2_df),3)
MCSE <- cbind(sqrt(select(estimates, B0_Cov:B2_Cov)*(100-select(estimates, B0_Cov:B2_Cov))/1825), sqrt(select(estimates, B0_Len:B2_Len)^2/1825))

val_MCSE <- matrix(paste(unlist(select(difference, B0_Cov:B2_Len), use.names = F), " (", round(unlist(MCSE,use.names = F), 3), ")", sep = ""),ncol = 6)
difference[,5:10] <- val_MCSE

bold <- difference[,5:10] > MCSE
difference[,5:10][bold] <- paste("**",difference[,5:10][bold],"**", sep = "")

rslt <- function(data, title) {
  data <- reshape(data, varying = 5:13, sep = "_", direction = "long", drop = c("var", "df"))
  data <- melt(select(data, m:B2), measure.vars = c("B0", "B1", "B2"), variable.name = "coefficients")
  data <- dcast(data, ... ~ ht + condition)
  names(data) <- c("N", "Measure", "Coefficient", "HM HC2 F_p", "HM HC2 n-p", "HM Sd_e n-p", "HT HC2 F_p", "HT HC2 n-p", "HT Sd_e n-p")
  
  
  rslttbl <- arrange(data, Coefficient, N, Measure)
  rslttbl$Parameter <- rep(" ", nrow(rslttbl))
  rslttbl$Est <- rslttbl$Parameter
  rslttbl <- rslttbl[,c("Parameter", 
                      "Coefficient", 
                      "N", 
                      "Measure", 
                      "Est",  
                      "HT Sd_e n-p",
                      "HT HC2 n-p", 
                      "HT HC2 F_p",
                      "HM Sd_e n-p", 
                      "HM HC2 n-p",  
                      "HM HC2 F_p")]
  rslttbl[1,6] <- paste(rslttbl[1,6],"&dagger;", sep = "")
  rslttbl[2,6] <- paste(rslttbl[2,6],"&Dagger;", sep = "")
  rslttbl[3,6] <- paste(rslttbl[3,6],"&sect;", sep = "")
  
  col1 <- c("Parameter", "n", " ", "Heteroscedasticity", "Homoscedasticity", NA)
  col2 <-c("", 
           "$\\hat{\\sigma}_i^2 =$",
           "$\\hat{\\sigma}_i^2$", 
           "$e_i^2/(1-h_i)$", 
           "$\\hat{\\sigma}_i^2$", 
           "$e_i^2/(1-h_i)$")
  col3 <- c(" ", 
            " ", 
            "$df_p =$", 
            "&nbsp;&nbsp;$n-p$&nbsp;&nbsp;&nbsp;", 
            "&nbsp;&nbsp;$n-p$&nbsp;&nbsp;&nbsp;", 
            "&nbsp;&nbsp;&nbsp;$\\hat{f}_p$&nbsp;&nbsp;&nbsp;", 
            "&nbsp;&nbsp;$n-p$&nbsp;&nbsp;&nbsp;", 
            "&nbsp;&nbsp;$n-p$&nbsp;&nbsp;&nbsp;", 
            "&nbsp;&nbsp;&nbsp;$\\hat{f}_p$&nbsp;&nbsp;&nbsp;")
  
  ncol1 <- c(1, 1, 1, 3, 3, NA)
  ncol2 <- c(2, 1, 1, 2, 1, 2)
  
  htmlTable(rslttbl[,-c(2,4)],
            rgroup = c("*$\\beta_0$*", "&nbsp;", "&nbsp;", "*$\\beta_1$*", "&nbsp;", "&nbsp;", "*$\\beta_2$*", "&nbsp;", "&nbsp;"),
            n.rgroup = rep(3,9),
            tfoot = c("&dagger;Coverage Probability", 
                      "&Dagger;Average Length of Confidence Interval", 
                      "&sect;Average Degrees-Of-Freedom"),
            cgroup = rbind(col1, col2),
            n.cgroup = rbind(ncol1, ncol2),
            caption = title,
            rnames = F,
            col.rgroup = c("none", "#F7F7F7"),
            header = col3)
}

```
Table 5 reports the findings of the current study. Table 6 reports the findings of Lipsitz and Ibrahim (1999) for comparison. Table 7 reports the differences between the current and original studies, as well as the Monte Carlo Standard Error (MCSE). Differences larger than the MCSE can be seen in bold. Of the `r dim(bold)[1]*dim(bold)[2]` estimates computed, `r sum(bold)` (`r round(sum(bold)/(dim(bold)[1]*dim(bold)[2]),3)*100`%) differences were larger than the MCSE.

```{r, cache = T}

rslt(estimates, "Table 5: Current study's coverage probabilities, average lengths and estimated degrees-of-freedom")
rslt(paper, "Table 6: Lipsitz and Ibrahim (1999) coverage probabilities, average lengths and estimated degrees-of-freedom")
rslt(difference, "Table 7: Table 5 - Table 6 (MCSE)")
```

###Discussion
The design of the study is effective in showing a proof of concept, however it has many limitations that primarily have to do with generalization and real world applications. The method by which the response string is generated guarantees a fair range of error variance. However such a uniform distribution is unlikely to be found in real data. This is especially problematic in the larger sample sizes because the response string is simply doubled or quadrupled. Although it would provide a less sterile result, generating random responses strings might be a more realistic approach in the future.

The model used to compute the estimates requires that researchers using these methods are aware of the correct model specifications in order to have accurate model fit (i.e. the quadratic term in $\beta_2x_i^2$). Yet model fit is never examined. This limits the scope of the simulation results greatly. An extension of this research could investigate coverage rates when the model is misspecified or includes more sets of covariates. 
