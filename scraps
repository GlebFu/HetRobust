Generalizations of HCCME are also available that provide asymptotically consistent variance estimates for auto-correlated errors \citep{Newey1987simple,Newey1994automatic} or errors that have an unknown dependence structure within clusters of observations \citep{Liang1986longitudinal}.

Yet another approach to approximating the distribution of test statistics based on HCCMEs is via bootstrap resampling. 
Recent attention has focused on a wild bootstrap technique proposed by \citet{Liu1988bootstrap}, which is valid under heteroskedasticity and provides substantially more accurate rejection rates than standard approaches in small samples \citep{Flachaire2005boostrapping, Davidson2008wild}. 
However, there are several nuances involved in implementing accurate wild bootstrap tests, including how to adjust the residuals, the choice of auxilliary distributions, and whether to bootstrap under a restricted model \citep{MacKinnon2013thirty}. 
In light of these additional considerations, as well as the computational intensity of simulations that involve resampling methods, the present investigation is limited to hypothesis testing procedures that do not involve resampling. 
In further work, we will investigate the performance of the best-performing methods identified in this paper compared to resampling tests such as wild bootstrapping and other recent proposals \citep[e.g.][]{Richard2016hetero}.

Although the squared residual $e_i^2$ is a poor estimate of $\sigma_i^2$ when considered in isolation, together the squared residuals provide an adequate means of estimating the middle term of Equation (\ref{eq:var_beta}). 


\subsection{\citet{MacKinnon2013thirty} design}

It is known that the performance of conventional tests based on HCCMEs is influenced not only by sample size, but by the distribution of the regressors \citep{Chesher1991finite, Cribari-Neto2004asymptotic, Kauermann2001note}. 
Specifically, observations with high leverage tend to distort the size of the conventional tests. 
In order to study the performance of HCCME-based tests under particularly challenging conditions, \citet{MacKinnon2013thirty} considered a regression with four log-normally distributed predictors, in which some observations have very high leverage. 
Following the same model, we simulated data according to the model in which the predictors $X_1,...,X_4$ are drawn independently from a standard log-normal distribution and the outcome follows the model
\[
Y_i = \beta_0 + \beta_1 X_{1i} + \cdots + \beta_4 X_{4i} + \sigma_i \epsilon_i,
\]
where $\epsilon_i \iid N(0, 1)$ and \[
\sigma_i = f(\zeta) \left(\beta_0 + \beta_1 X_{1i} + \cdots + \beta_4 X_{4i}\right)^\zeta.
\] 
The constant $\zeta$ controls the degree of heteroskedasticity, with $\zeta = 0$ corresponding to homoskedasticity and $\zeta = 2$ representing quite extreme heteroskedasticity. 
The scaling factor $f(\zeta)$ is chosen so that the average variance is held constant (i.e., $\E\left(\sigma_i^2\right) = 1$).
Following \citet{MacKinnon2013thirty}, we set $\beta_0 = \cdots = \beta_3 = 1$, $\beta_4 = 0$ and test $H_0: \beta_4 = 0$. 

Based on this model, we simulated samples varying in size from 20 to 200, using $\gamma = 0,1,2$. For each simulated dataset, we calculated robust t-statistics using the HC0, HC1, HC2, HC3, HC4, HC4m, and HC5 adjustment factors and corresponding critical values based on the Satterthwaite approximation, both Edgeworth approximations from \citet{Kauermann2001note}, the \citet{Rothenberg1988approximate} Edgeworth approximation, and the saddlepoint approximation, as well as the conventional $t(n - p)$ critical values. For all but the conventional approximation, we examined both empirical- and model-based versions of the correction. We considered nominal type-I error levels of $\alpha = .005$, $.010$, and $.050$. For each combination of parameters, empirical rejection rates are estimated from 10,000 replications.

\subsection{\citet{Long2000using} design}

We simulated data from a model with four regressors. Following \citet{Long2000using}, we first simulated independent variates $\delta_1 \sim \text{Unif}(0, 1)$, $\delta_2 \sim N(0,1)$, $\delta_3 \sim \chi^2_1$, $\delta_4 \sim N(0,1)$, and $\delta_5 \sim \text{Unif}(0, 1)$. These were then combined as
\begin{align*}
x_1 &= 1 + \delta_1 \\
x_2 &= 3 \delta_1 + 0.6 \delta_2 \\
x_3 &= 2 \delta_1 + 0.6 \delta_3 \\
x_4 &= 0.1 \delta_1 + 0.9 \delta_3 - 0.8 \delta_4 + 4 \delta_5,
\end{align*}
producing regressors with inter-correlations ranging from 0.27 to 0.82.\todo{Gleb, why do we truncate $x_2$ and $x_4$ at -2.5?} We then generated the outcome as
\[
y = 1 + x_1 + x_2 + x_3 + 0 \times x_4 + f_j(\mathbf{x}) \zeta,
\]
where $\zeta$ followed one of three possible distributions: $N(0,1)$, $\chi^2_5$, or $t_5$. The $f_j(\mathbf{x})$ was one of seven different error structures that created different patterns of heteroskedasticity:
\begin{itemize}
\item $f_0(\mathbf{x}) = 1$, which produced a model with homoskedastic errors;
\item $f_1(\mathbf{x}) = \sqrt{x_1}$;
\item $f_2(\mathbf{x}) = \sqrt{x_3 + 1.6}$;
\item $f_3(\mathbf{x}) = \sqrt{x_3(x_4 + 2.5)}$;
\item $f_4(\mathbf{x}) = \sqrt{x_1 x_3 (x_2 + 2.5)}$;
\item $f_5(\mathbf{x}) = 1 + 0.5 I(x_2 > 1.6)$; or
\item $f_6(\mathbf{x}) = 1 + 3 I(x_2 > 1.6)$.
\end{itemize}
We simulated samples of size $n = 25$, 50, 100, 250, and 500. 

For each simulated dataset, we estimated model (\ref{eq:regression_model}). In order to estimate true Type-I error rates, we tested the hypotheses that $\beta_j = 1$ for $j = 0,...,3$ and $\beta_4 = 0$. To estimate power, we tested the hypotheses that $\beta_j = 0$ for $j = 0,...,3$. We considered nominal type-I error levels of $\alpha = .01$, $.05$, and $.10$. We tested each hypothesis using 17 different procedures. First, we calculated tests based on the HC0, HC1, HC2, HC3, HC4, HC4m, and HC5 adjustment factors compared to conventional $t(n - p)$ critical values. Second, we calculated tests using the Satterthwaite approximation (with HC2), both Edgeworth approximations from \citet[][also using HC2]{Kauermann2001note}, the \citet{Rothenberg1988approximate} Edgeworth approximation (with HC0), and the saddlepoint approximation (with HC2). For each of the distributional approximations, we examined both empirical- and model-based versions of the correction. Empirical rejection rates were estimated from 20,000 replications.


\newpage
\begin{center}
{\large\bf SUPPLEMENTARY MATERIAL}
\end{center}

\begin{description}

\item[Title:] Brief description. (file type)

\item[R-package for  MYNEW routine:] R-package ?MYNEW? containing code to perform the diagnostic methods described in the article. The package also contains all datasets used as examples in the article. (GNU zipped tar file)

\item[HIV data set:] Data set used in the illustration of MYNEW method in Section~ 3.2. (.txt file)

\end{description}