\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%



\newcommand{\Prob}{\text{Pr}}
\newcommand{\E}{\text{E}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\corr}{\text{corr}}
\newcommand{\Var}{\text{Var}}
\newcommand{\tr}{\text{tr}}
\newcommand{\iid}{\stackrel{\text{iid}}{\sim}}
\newcommand{\diag}{\text{diag}}
\newcommand{\bm}{\mathbf}
\newcommand{\bs}{\boldsymbol}

\newcommand{\MyTitle}{Heteroskedasticity-robust tests of linear regression coefficients: A review and evaluation of small-sample corrections}
\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf \MyTitle}
  \author{James E. Pustejovsky and Gleb Furman \\
    University of Texas at Austin \\
    Educational Psychology Department}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf \MyTitle}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
The text of your abstract.  200 or fewer words.
\end{abstract}

\noindent%
{\it Keywords: heteroskedasticity; sandwich estimator; robust covariance estimator; linear regression; Satterthwaite approximation; saddlepoint approximation; Edgeworth approximation} 
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\section{Introduction}
\label{sec:intro}

Linear regression models, estimated by ordinary least squares (OLS), are one of the most important and ubiquitous tools in applied statistical work.
Classically, hypothesis tests and confidence intervals for linear regression coefficients rely on the assumption that the model errors are homoskedastic, or have constant variance for all values of the covariates. 
However, it can be difficult to diagnose violations of this assumption---particularly in small samples---and so it is often desirable to use methods that do not rely on it. 
One common solution is to use a heteroskedasticity-consistent covariance matrix estimator (HCCME), which provides an asymptotically consistent estimate of the sampling variance of OLS coefficient estimates for models with heteroskedasticity of an unknown form. 
Generalizations of HCCME are also available that provide asymptotically consistent variance estimates for auto-correlated errors \citep{Newey1987simple,Newey1994automatic} or errors that have an unknown dependence structure within clusters of observations \citep{Liang1986longitudinal}.

HCCMEs were introduced by \citet{Huber1967behavior}, \citet{Eicker1967limit}, and \citet{White1980heteroskedasticity}. \citet{MacKinnon1985some} proposed several variations to improve the finite-sample properties of the HCCMEs. 
In a large simulation study, \citet{Long2000using} demonstrated that one of these variations, HC3, performs substantially better than the others.
HC3 is the default in software such as the R package \texttt{sandwich} \citep{Zeileis2004econometric}, although the original HCCME (commonly called HC0) remains the default in SAS \texttt{proc reg} and Stata's \texttt{regress} command with \texttt{vce(robust)}. 
More recently, several further variations on the HCCMEs have been proposed \citep{Cribari-Neto2004asymptotic, Cribari-Neto2007inference, Cribari-Neto2011new}. 
For hypothesis testing, HCCMEs are typically used to calculate t-statistics, which are compared to standard normal or $t(n - p)$ reference distributions, where $n$ is the sample size and $p$ is the dimension of the coefficient vector. 

Another approach to improving the small-sample properties of hypothesis tests based on HCCMEs is to find a better approximation to the null sampling distribution of the test statistic. 
Several such approximations have been proposed, including Satterthwaite approximations \citep{Lipsitz1999degrees}, Edgeworth approximations \citep{Rothenberg1988approximate, Kauermann2001note}, and saddlepoint approximations \citep{McCaffrey2006improved}. 
Although there is evidence that each of these approximations improves upon the standard, large-sample tests, their performance has been examined only under a limited range of conditions. 
Moreover, it appears that these approximations have been developed independently and without reference to previous work, and their performance has never been compared under a common set of conditions. 
In contrast to the various HC corrections, to our knowledge, none of the distributional approximations are implemented in standard software packages for data analysis. 

In this paper, we review the various small-sample approximations for hypothesis tests based on HCCMEs, using a common notation in order to facilitate comparisons among them. In so doing, we identify several further variations on the approximations that have not previously been considered. We then evaluate the performance of these approximations, along with the standard methods, in a large simulation study. The design of the simulation study is modeled on the earlier study of \citet{Long2000using}.

\section{Methods}
\label{sec:meth}

\subsection{Model}

We will consider the regression model
\begin{equation}
y_i = \bm{x}_i\bs\beta + \epsilon_i,
\end{equation}
for $i = 1,...,n$, where $y_i$ is the outcome, $\bm{x}_i$ is a $1 \times p$ row-vector of covariates (including an intercept) for observation $i$, $\bs\beta = (\beta_1,...,\beta_p)'$ is a $p \times 1$ vector of regression coefficients, and $\epsilon_i$ is a mean-zero error term with variance $\sigma_i^2$. We shall assume that the errors are mutually independent. For ease of notation, let $\bm{y} = \left(y_1,...,y_n\right)'$ denote the $n \times 1$ vector of outcomes, $\bm{X} = \left(\bm{x}_1',...,\bm{x}_n'\right)'$ be the $n \times p$ design matrix, and $\bs\epsilon$ be the $n \times 1$ vector of errors with $\E\left(\bs\epsilon\right) = \bm{0}$ and $\Var\left(\bs\epsilon\right) = \bs\Sigma = \diag\left(\sigma_1^2,...,\sigma_n^2\right)$. Let $\bm{M} = \left(\bm{X}'\bm{X} / n\right)^{-1}$. Finally, let $\bs{\hat\beta} = \left(\bm{M}\right)\bm{X}'\bm{y} / n$ denote the vector of OLS estimates and $e_i = y_i - \bm{x}_i \bs{\hat\beta}$ denote the residual for unit $i$. 

The goal is to test a hypothesis regarding a linear combination of the regression coefficients $\bm{c}'\bs\beta$, i.e., $H_0: \bm{c}'\bs\beta = k$, with Type-I error rate $\alpha$. All tests under consideration are based on the Wald statistic
\begin{equation}
T(\bm{V}) = \frac{\bm{c}'\bs{\hat\beta} - k}{\sqrt{\bm{c}' \bm{V} \bm{c}}},
\end{equation}
where $\bm{V}$ is some estimator for $\Var\left(\bs{\hat\beta}\right)$. 

If the errors are homoskedastic, so that $\sigma_i^2 = \sigma^2$ for $i = 1,...,n$, then the hypothesis can be tested using a standard t-test. The variance of $\bs\beta$ is then estimated by $\bm{V}^{hom} = \hat\sigma^2 \bm{M}$, where $\hat\sigma^2 = \left(\sum_{i=1}^n e_i^2\right) / (n - p)$. Under $H_0$ and assuming that the errors are normally distributed and homoskedastic, the t-statistic follows a $t$ distribution with $n - p$ degrees of freedom. Thus, $H_0$ is rejected if $\left|T\left(\bm{V}^{hom}\right)\right| > F_t^{-1}\left(1 - \frac{\alpha}{2}; n - p\right)$, where $F_t^{-1}(x; \nu)$ is the quantile function for a $t$ distribution with $\nu$ degrees of freedom. However, if the errors are instead heteroskedastic, the variance estimator $\bm{V}^{hom}$ will be inconsistent and this t-test will generally have incorrect size. 

\subsection{HCCMEs}

Under the general model that allows for heteroskedasticity, the true variance of the OLS estimator is 
\begin{equation}
\label{eq:var_beta}
\Var\left(\bs{\hat\beta}\right) = \frac{1}{n} \bm{M} \left(\frac{1}{n}\sum_{i=1}^n \sigma_i^2 \bm{x}_i\bm{x}'\right) \bm{M}
\end{equation}
The HCCMEs all involve estimating $\Var\left(\bs{\hat\beta}\right)$ by replacing the $\sigma_i^2$ with crude estimates involving the squared residuals. Although taken singly, the squared residual $e_i^2$ is a poor estimate of $\sigma_i^2$, together the residuals provide an adequate means of estimating the middle term of Equation (\ref{eq:var_beta}). The HCCCMEs all have the general form 
\begin{equation}
\label{eq:sandwich}
\bm{V}^{HC} = \frac{1}{n} \bm{M} \left(\frac{1}{n}\sum_{i=1}^n \omega_i e_i^2 \bm{x}_i\bm{x}'\right)\bm{M} 
\end{equation}
where $\omega_{1},...,\omega_{n}$ are correction terms that differ for the various HC estimators. Under weak assumptions, the weak law of large numbers ensures that the middle term in Equation (\ref{eq:sandwich}) converges to the corresponding term in (\ref{eq:var_beta}) as the sample size increases. Furthermore, the robust Wald statistic $T\left(\bm{V}^{HC}\right)$ converges in distribution to $N(0,1)$ as $n$ increases to infinity. Thus, any asymptotically correct test can be constructed by rejecting $H_0$ when $\left|T\left(\bm{V}^{HC}\right)\right|$ is greater than the $1 - \alpha / 2$ critical value from a standard normal distribution. Because this test often has inflated size in small samples, it is common to instead use the critical value from a $t$ distribution with $n - p$ degrees of freedom.

\subsection{Rothenberg's Edgeworth approximation}

\citet{Rothenberg1988approximate} developed an Edgeworth approximation for the distribution of Wald-type $t$-statistics based on the HC0 variance estimator.
It is straight-forward to generalize the approach to any of the HC estimators.
Let
\begin{align*}
g_i &= \bm{x}_i\bm{M}\bm{c} \\
f_i &= n \bm{x}_i \bm{M} \bm{X}' \bs\Sigma \bm{X}\bm{M}\bm{c} \\ 
q_i &= \bm{x}_i\bm{M}\bm{X}'\bs\Sigma \bm{X}\bm{M}\bm{x}_i' \\
a &= \frac{\sum_{i=1}^n \omega_i g_i^2 z_i^2}{\left(\sum_{i=1}^n g_i^2 \sigma_i^2\right)^2} \\
b &= \frac{\sum_{i=1}^n \omega_i g_i^2  q_{ii}}{\sum_{i=1}^n g_i^2 \sigma_i^2} \\
\nu &= \frac{2\left(\sum_{i=1}^n g_i^2 \sigma_i^2\right)^2}{\sum_{i=1}^n \omega_i^2 g_i^4 \sigma_i^4}
\end{align*}
For an observed value of the test statistic $t_{HC}$, the corresponding p-value is calculated as \[
p = 2 \left[1 - \Phi\left[\frac{\left|t_{HC}\right|}{2}\left(2 - \frac{1 + t_{HC}^2}{\nu} + a\left(t_{HC}^2 - 1\right) + b\right)\right]\right], \]
where $\Phi(\cdot)$ is the standard normal cumulative distribution function. 
A further approximation provides a means for calculating a critical value for a specified $\alpha$-level. Let $z_{\alpha}$ denote the $1 - \alpha / 2$ quantile from a standard normal distribution. Here, the hypothesis test is rejected if $t_{HC}$ is greater than the critical value $t_{crit}$ defined by \[
t_{crit} = \frac{z_{\alpha}}{2}\left[2 + \frac{z_{\alpha}^2 + 1}{\nu} - a\left(z_{\alpha}^2 - 1\right) - b\right]. \]
In practice, these testing procedures will need to be based on estimates of the quantities involved. Rothenberg proposed a simple estimate of the degrees of freedom: \[
\nu_q = \frac{6\left(\sum_{i=1}^n \omega_i g_i^2 e_i^2\right)^2}{\sum_{i=1}^n \omega_i^2 g_i^4 e_i^4}.
\]
Rothenberg also proposed to calculate $a$, $b$, $\bm{z}_q$, and $\bm{Q}$ by simply replacing the values of $\sigma_i^2$ with $\omega_i e_i^2$. Alternately, one could assume that $\bs\Sigma = \sigma^2 \bm{I}$, in which case $\bm{z} = \bm{0}$, $a = 0$, 
\[
b = - \frac{\sum_{i=1}^n h_i \omega_i g_i^2}{\sum_{i=1}^n g_i^2}, \qquad \text{and} \qquad 
\nu = \frac{2\left(\sum_{i=1}^n g_i^2 \right)^2}{\sum_{i=1}^n \omega_i^2 g_i^4}. 
\]

\subsection{Kauermann and Carroll's Edgeworth approximation}

\citet{Kauermann2001note}

\subsection{Satterthwaite approximation} 

\citet{Lipsitz1999degrees}

\subsection{Saddlepoint approximation}

\citet{McCaffrey2006improved}

\section{Simulation study}
\label{sec:verify}

\section{Conclusion}
\label{sec:conc}


\newpage
\begin{center}
{\large\bf SUPPLEMENTARY MATERIAL}
\end{center}

\begin{description}

\item[Title:] Brief description. (file type)

\item[R-package for  MYNEW routine:] R-package ?MYNEW? containing code to perform the diagnostic methods described in the article. The package also contains all datasets used as examples in the article. (GNU zipped tar file)

\item[HIV data set:] Data set used in the illustration of MYNEW method in Section~ 3.2. (.txt file)

\end{description}

\bibliographystyle{agsm}
\bibliography{Bibliography}

\end{document}
