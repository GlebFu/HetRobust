\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\usepackage[textwidth=1in, textsize=tiny]{todonotes}


\newcommand{\Prob}{\text{Pr}}
\newcommand{\E}{\text{E}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\corr}{\text{corr}}
\newcommand{\Var}{\text{Var}}
\newcommand{\tr}{\text{tr}}
\newcommand{\iid}{\stackrel{\text{iid}}{\sim}}
\newcommand{\diag}{\text{diag}}
\newcommand{\bm}{\mathbf}
\newcommand{\bs}{\boldsymbol}

\newcommand{\MyTitle}{Heteroskedasticity-robust tests of linear regression coefficients: A review and evaluation of small-sample corrections}
\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf \MyTitle}
  \author{James E. Pustejovsky and Gleb Furman \\
    University of Texas at Austin \\
    Educational Psychology Department}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf \MyTitle}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
The text of your abstract.  200 or fewer words.
\end{abstract}

\noindent%
{\it Keywords: heteroskedasticity; sandwich estimator; robust covariance estimator; linear regression; Satterthwaite approximation; saddlepoint approximation; Edgeworth approximation} 
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\section{Introduction}
\label{sec:intro}

Linear regression models, estimated by ordinary least squares (OLS), are one of the most important and ubiquitous tools in applied statistical work.
Classically, hypothesis tests and confidence intervals for linear regression coefficients rely on the assumption that the model errors are homoskedastic, or have constant variance for all values of the covariates. 
In practice though, it can be difficult to diagnose violations of this assumption, and similarly difficult to construct and defend other assumptions about how the error variances related to the covariates. 
Thus, it is often desirable to use methods of inference that remain valid for models with heteroskedasticity of an unknown form.\todo{Connect to Behrens-Fisher problem and cluster-robust variance estimation?} 

One well-known approach to inference in this setting is based on heteroskedasticity-consistent covariance matrix estimators (HCCMEs), which provide asymptotically consistent estimates of the sampling variance of OLS coefficient estimates under quite general conditions. HCCMEs were introduced in the statistics literature by \citet{Huber1967behavior} and \citet{Eicker1967limit}, in the survey sampling literature by\todo{add citations} and in econometrics by \citet{White1980heteroskedasticity}. They are an attractive tool because they rely on weaker assumptions than classical methods. However, they also have the drawback that it is not always clear whether a given sample is sufficently large to trust the asymptotic approximations by which they are warranted. Furthermore, when the sample size is small, it is known that some of the HCCMEs tend to be too liberal, producing variance estimates that are biased towards zero and hypothesis tests with greater than nominal size \citep{Long2000using}. 

Since \citet{White1980heteroskedasticity} introduced the HCCME in econometrics, methods for improving the finite-sample properties of HCCMEs have been studied extensively.  
The most well-known strand of this work has considered modifications to the HCCME itself that lead to more accurate tests and CIs in finite samples. \citet{MacKinnon1985some} and \citet{Davidson1993estimation} proposed several such modifications that are now readily available in software. 
Based upon an extensive set of simulations, \citet{Long2000using} demonstrated that one of these modifications, known as HC3, performs substantially better than the others.
As a result, HC3 is the default in software such as the R package \texttt{sandwich} \citep{Zeileis2004econometric}, although White's original HCCME remains the default in SAS \texttt{proc reg} and Stata's \texttt{regress} command with \texttt{vce(robust)}. 
More recently, several further variations on the HCCMEs have been proposed \citep{Cribari-Neto2004asymptotic, Cribari-Neto2007inference, Cribari-Neto2011new}, which aim to improve upon the performance of HC3 in models where the regressors exhibit high leverage. 
For hypothesis testing, HCCMEs are typically used to calculate t-statistics, which are compared to standard normal or $t(n - p)$ reference distributions, where $n$ is the sample size and $p$ is the dimension of the coefficient vector.

An alternative approach to improving the small-sample properties of hypothesis tests based on HCCMEs is to find a better approximation to the null sampling distribution of the test statistic. 
Several such approximations have been proposed, including Satterthwaite approximations \citep{Lipsitz1999degrees}, Edgeworth approximations \citep{Rothenberg1988approximate, Kauermann2001note}, and saddlepoint approximations \citep{McCaffrey2006improved}. 
Although there is evidence that each of these approximations improves upon the standard, large-sample tests, their performance has been examined only under a limited range of conditions. 
Moreover, it appears that these approximations have been developed in isolation, without reference to previous work, and they have received little or no attention in recent reviews \citep[e.g., none are discussed by][]{MacKinnon2013thirty}. 
In contrast to the various HC corrections, to our knowledge, none of the distributional approximations are implemented in standard software packages for data analysis.\todo{Add more on gaps in literature.} 

In this paper, we review the various small-sample approximations for hypothesis tests based on HCCMEs, using a common notation in order to facilitate comparisons among them. In so doing, we identify several further variations on the approximations that have not previously been considered. We then evaluate the performance of these approximations, along with the standard methods, in a large simulation study. The design of the simulation study is modeled on the earlier study of \citet{Long2000using}.\todo{Outline paper}

\section{Methods}
\label{sec:model}

We will consider the regression model
\begin{equation}
y_i = \bm{x}_i\bs\beta + \epsilon_i,
\end{equation}
for $i = 1,...,n$, where $y_i$ is the outcome, $\bm{x}_i$ is a $1 \times p$ row-vector of covariates (including an intercept) for observation $i$, $\bs\beta$ is a $p \times 1$ vector of regression coefficients, and $\epsilon_i$ is a mean-zero error term with variance $\sigma_i^2$. 
We shall assume that the errors are mutually independent. 
For ease of notation, let $\bm{y} = \left(y_1,...,y_n\right)'$ denote the $n \times 1$ vector of outcomes, $\bm{X} = \left(\bm{x}_1',...,\bm{x}_n'\right)'$ be the $n \times p$ design matrix, and $\bs\epsilon$ be the $n \times 1$ vector of errors with $\E\left(\bs\epsilon\right) = \bm{0}$ and $\Var\left(\bs\epsilon\right) = \bs\Sigma = \diag\left(\sigma_1^2,...,\sigma_n^2\right)$. 
Let $\bm{M} = \left(\bm{X}'\bm{X} / n\right)^{-1}$. 
Let $\bs{\hat\beta} = \bm{M}\bm{X}'\bm{y} / n$ denote the vector of OLS estimates and $e_i = y_i - \bm{x}_i \bs{\hat\beta}$, $i = 1,...,n$ denote the residuals. 

The goal is to test a hypothesis regarding a linear combination of the regression coefficients $\bm{c}'\bs\beta$, i.e., $H_0: \bm{c}'\bs\beta = k$, with Type-I error rate $\alpha$. 
All tests under consideration are based on the Wald statistic
\begin{equation}
T(\bm{V}) = \frac{\bm{c}'\bs{\hat\beta} - k}{\sqrt{\bm{c}' \bm{V} \bm{c}}},
\end{equation}
where $\bm{V}$ is some estimator for $\Var\left(\bs{\hat\beta}\right)$. 

If the errors are homoskedastic, so that $\sigma_i^2 = \sigma^2$ for $i = 1,...,n$, then the hypothesis can be tested using a standard t test. 
The variance of $\bs\beta$ is then estimated by $\bm{V}^{hom} = \hat\sigma^2 \bm{M}$, where $\hat\sigma^2 = \left(\sum_{i=1}^n e_i^2\right) / (n - p)$. 
Under $H_0$ and assuming that the errors are normally distributed, the test statistic follows a $t$ distribution with $n - p$ degrees of freedom. 
Thus, $H_0$ is rejected if $\left|T\left(\bm{V}^{hom}\right)\right| > F_t^{-1}\left(1 - \frac{\alpha}{2}; n - p\right)$, where $F_t^{-1}(x; \nu)$ is the quantile function for a $t$ distribution with $\nu$ degrees of freedom. 
If the errors are instead heteroskedastic, the variance estimator $\bm{V}^{hom}$ will be inconsistent and this t test will generally have incorrect size. 

\subsection{HCCMEs}

Under the general model that allows for heteroskedasticity, the true variance of the OLS estimator is 
\begin{equation}
\label{eq:var_beta}
\Var\left(\bs{\hat\beta}\right) = \frac{1}{n} \bm{M} \left(\frac{1}{n}\sum_{i=1}^n \sigma_i^2 \bm{x}_i\bm{x}'\right) \bm{M}
\end{equation}
The HCCCMEs estimate $\Var\left(\bs{\hat\beta}\right)$ by replacing the $\sigma_i^2$ with estimates involving the squared residuals. They all have the same general form:  
\begin{equation}
\label{eq:sandwich}
\bm{V}^{HC} = \frac{1}{n} \bm{M} \left(\frac{1}{n}\sum_{i=1}^n \omega_i e_i^2 \bm{x}_i\bm{x}'\right)\bm{M} 
\end{equation}
where $\omega_{1},...,\omega_{n}$ are weighting terms that differ for the various HC estimators. 
Under weak assumptions, the weak law of large numbers ensures that the middle term in Equation (\ref{eq:sandwich}) converges to the corresponding term in (\ref{eq:var_beta}) as the sample size increases. 

\citet{White1980heteroskedasticity} originally described the HCCME without any correction factor, which is equivalent to taking $\omega_i = 1$ for $i = 1,...,n$. 
This form has come to be known as HC0. 
Subsequently, various correction factors have been proposed that aim to improve on the finite-sample behavior of HC0. 
Following common convention, we refer to these correction factors by number. Letting $h_i = n \bm{x}_i \bm{M} \bm{x}_i'$ denote the hat value for unit $i$, the correction factors are as follows:
\begin{align*}
\text{HC1:} \qquad \omega_i &= n / (n - p) \\
\text{HC2:} \qquad \omega_i &= (1 - h_i)^{-1} \\
\text{HC3:} \qquad \omega_i &= (1 - h_i)^{-2} \\
\text{HC4:} \qquad \omega_i &= (1 - h_i)^{-\delta}, \qquad \delta_i = \min\{h_i n / p, 4\} \\
\text{HC4m:} \qquad \omega_i &= (1 - h_i)^{-\delta}, \qquad \delta_i = \min\left\{h_i n / p, 1 \right\} + \min\left\{h_i n / p, 1.5 \right\} \\
\text{HC5:} \qquad \omega_i &= (1 - h_i)^{-\delta}, \qquad \delta_i = \frac{1}{2}\min\left\{h_i n / p, \max \left\{4, 0.7 h_{(n)} n / p\right\}\right\}
\end{align*}
\citet{MacKinnon1985some} suggested HC1, which uses an ad hoc correction similar to the correction used for $\hat\sigma^2$, and HC2, which has the property that $\bm{V}^{HC2}$ is exactly unbiased when the errors are homoskedastic. 
\citet{Davidson1993estimation} proposed HC3 as an approximation to the leave-on-out jackknife variance estimator. 

Cribari-Neto and colleagues subsequently proposed three further variations, HC4 \citep{Cribari-Neto2004asymptotic}, HC4m \citep{Cribari-Neto2011new}, and HC5 \citep{Cribari-Neto2007inference}, all of which aim to improve upon HC3 for design matrices where some observations are very influential. 
All of these correction factors inflate the squared residual terms to a greater extent when the observation has a higher degree of leverage. 
HC4 truncates the degree of inflation at 4 times the average leverage. 
Compared to HC4, HC4m inflates observations with lower leverage more strongly, but it also truncates the maximum degree of inflation at 2.5 times the average. 
In HC5, the truncation depends on the maximum leverage value but the degree of inflation will tend to be smaller than HC4. 

For any of the HCCMEs, the robust Wald statistic $T\left(\bm{V}^{HC}\right)$ converges in distribution to $N(0,1)$ as $n$ increases to infinity. 
Thus, any asymptotically correct test can be constructed by rejecting $H_0$ when $\left|T\left(\bm{V}^{HC}\right)\right|$ is greater than the $1 - \alpha / 2$ critical value from a standard normal distribution. 
In practice, it is common to instead use the critical value from a $t$ distribution with $n - p$ degrees of freedom. 
However, use of the $t_{n-p}$ reference distribution is only an ad hoc approximation.

The following subsections review several different, better-grounded approximations to the null sampling distribution of $T\left(\bm{V}^{HC}\right)$. As will be seen, its distribution depends on the structure of the error variances, which are unknown and must thus be estimated. One of the 

\subsection{Satterthwaite approximation} 

\citet{Lipsitz1999degrees} proposed a small-sample corrected hypothesis testing procedure that is based on a Satterthwaite approximation for the distribution of $V^{HC2}_{qq}$. The Satterthwaite approximation involves approximating the distribution of $V^{HC2}_{qq}$ by a multiple of a $\chi^2$ distribution with degrees of freedom $2 \left[\E\left(V^{HC2}_{qq}\right)\right]^2 / \Var\left(V^{HC2}_{qq}\right)$. In practice, the mean and variance must be estimated because they involve the unknown quantity $\bs\Sigma$. Lipsitz and colleagues note that the variance of $V^{HCx}_{qq}$ can be written as \[
\Var\left(V^{HCx}_{qq}\right) = 2\tr\left[\left(\bm{I} - \bm{H}\right)\bm{A}_{xq} \left(\bm{I} - \bm{H}\right)\left[\left(\left(\bm{I} - \bm{H}\right)\bm{A}_{xq} \left(\bm{I} - \bm{H}\right)\right)\circ \bm{S}\right]\right], \]
where $\circ$ denotes the element-wise (Hadamard) product and $\bm{S}$ has entries $S_{ij} = \sigma_i^2 \sigma_j^2$. They propose to estimate $\bm{S}$ using the matrix with entries \[
\hat{S}_{ii} = \frac{e_i^4}{3(1 - h_{ii})^2} \quad \text{for } i = 1,...,n \quad \text{and} \quad \hat{S}_{ij} = \frac{e_i^2 e_j^2}{2 h_{ij}^2 + (1 - h_{ii})(1 - h_{jj})} \quad \text{for } i \neq j. \]
They then construct estimated degrees of freedom $\nu_q$ by substituting $V^{HC2}_{qq}$ in place of its expectation and taking 
\begin{equation}
\label{eq:Satterthwaite}
\nu_q = \frac{\left(V^{HC2}_{qq}\right)^2}{\tr\left[\left(\bm{I} - \bm{H}\right)\bm{A}_{2q} \left(\bm{I} - \bm{H}\right)\left[\left(\left(\bm{I} - \bm{H}\right)\bm{A}_{2q} \left(\bm{I} - \bm{H}\right)\right)\circ \bm{\hat{S}}\right]\right]}.
\end{equation}
The null hypothesis is tested by comparing $t^{HC2}_q$ to a $t$-distribution with $\nu_q$ degrees of freedom, i.e., $H_0$ is rejected if $|t^{HC2}_q| > F_t^{-1}(1 - \alpha / 2; \nu_q)$. Equivalently, the $p$-value corresponding to $H_0$ is $2 \left[1 - F_t\left(|t_q^{HC2}|; \nu_q\right)\right]$, where $F_t(x; \nu)$ is the cumulative distribution function of a $t_\nu$ distribution.

\subsection{Kauermann and Carroll's Edgeworth approximation}

\citet{Kauermann2001note} proposed a method of constructing confidence intervals based on HC variance estimators that is based on a somewhat simpler Edgeworth approximation. The hypothesis testing procedure corresponding to their proposed confidence intervals rejects the null if $\left|t_{HC} \right| > z_{\tilde\alpha}$, where $\tilde\alpha$ is implicitly defined as the solution to \begin{equation}
\label{eq:Kauermann_crit}
\alpha = \tilde\alpha + \frac{\phi\left(z_{\tilde\alpha}\right)}{2 \nu_q}\left(z_{\tilde\alpha}^3 + z_{\tilde\alpha}\right), 
\end{equation}
where $\phi(\cdot)$ is the density of the standard normal distribution and \[
\nu_q  = \frac{2 \left[\Var(\hat\beta_q)\right]^2}{\Var\left(V^{HC}_{qq}\right)}\] 
is a degrees of freedom measure. Equivalently, the $p$-value for the test is given by \[
p = 2 \left[1 - \Phi\left(|t^{HC}_q|\right)\right] + \frac{\phi\left(t^{HC}_q\right)}{2 \nu_q}\left(\left|t^{HC}_q\right|^3 + \left|t^{HC}_q\right|\right), \]
These authors also offer a further approximation for the critical value $z_{\tilde\alpha}$, which saves the trouble of solving Equation (\ref{eq:Kauermann_crit}):
\[
z_{\tilde\alpha} = F_t^{-1}\left(1 - \frac{\alpha}{2}; n - p\right) + \frac{\left(z_\alpha^3 + z_\alpha\right)}{4}\left(\frac{1}{\nu_q} - \frac{\left(\sum_{i=1}^n g_{qi}^2\right)^2}{n}\right). \]

In contrast to the degrees of freedom estimator used by Lipsitz and colleagues (as given in Equation \ref{eq:Satterthwaite}), Kauermann and Carroll calculate the degrees of freedom under the working assumption that the errors are actually homoskedastic. Under this working assumption, the HC2 variance estimator is unbiased, with degrees of freedom are given by \[
\nu_q = \left(\sum_{i=1}^n g_{qi}^2\right)^2\left(\sum_{i=1}^n g_{qi}^4 + \sum_{i=1}^n \sum_{j \neq i} \frac{g_{qi}^2 g_{qj}^2 h_{ij}^2}{(1 - h_{ii})(1 - h_{jj})}\right)^{-1} \].

\subsection{Rothenberg's Edgeworth approximation}

\citet{Rothenberg1988approximate} developed an Edgeworth approximation for the distribution of Wald-type t-statistics under the assumption that the errors are normally distributed. The original approximation was developed for tests based on the HC0 variance estimator, but extending it other HCCMEs is straightforward; here, we state the more general form. 
Let
\begin{align*}
g_i &= \bm{x}_i\bm{M}\bm{c} \\
z_i &= \sigma_i^2 g_i - \bm{x}_i \bm{M}\bm{X}'\bs\Sigma \bm{X} \bm{M}\bm{c} / n \\ 
q_i &= \frac{1}{n^2} \bm{x}_i\bm{M}\bm{X}'\bs\Sigma \bm{X}\bm{M}\bm{x}_i'  - 2 h_i \\
a &= \frac{\sum_{i=1}^n \omega_i g_i^2 z_i^2}{\left(\sum_{i=1}^n g_i^2 \sigma_i^2\right)^2} \\
b &= \frac{\sum_{i=1}^n \omega_i g_i^2  q_i}{\sum_{i=1}^n g_i^2 \sigma_i^2} \\
\nu &= \frac{2\left(\sum_{i=1}^n g_i^2 \sigma_i^2\right)^2}{\sum_{i=1}^n \omega_i^2 g_i^4 \sigma_i^4}
\end{align*}
Rothenberg's Edgeworth approximation is then given by 
\[
\Pr\left(T(\bm{V}^{HC}) \leq t \right) \approx \Phi\left[t \left(1 - \frac{1 + t^2}{2\nu} + \frac{a\left(t^2 - 1\right) + b}{2}\right)\right], 
\]
where $\Phi(\cdot)$ is the standard normal cumulative distribution function.
Based on the Edgeworth approximation, \citet{Rothenberg1988approximate} proposed a test in which the null hypothesis is rejected if the observed test statistic is greater than the critical value defined by 
\begin{equation}
\label{eq:edge_Roth}
t_{crit} = z_{\alpha}\left[1 + \frac{z_{\alpha}^2 + 1}{2 \nu} - \frac{a\left(z_{\alpha}^2 - 1\right) + b}{2}\right],
\end{equation}
where $z_\alpha$ is the $1 - \alpha / 2$ standard normal critical value. 

In practice, the quantities $a$, $b$, and $\nu$ must be estimated because they depend on the unknown error variances. 
Rothenberg proposed to do so by replacing values of $\sigma_i^2$ with $\omega_i e_i^2$ and values of $\sigma_i^4$ with $\omega_i^2 e_i^4 / 3$. 
Alternately, one could assume that $\bs\Sigma = \sigma^2 \bm{I}$, in which case $a = 0$, 
\[
b = - \frac{\sum_{i=1}^n h_i \omega_i g_i^2}{\sum_{i=1}^n g_i^2}, \qquad \text{and} \qquad 
\nu = \frac{2\left(\sum_{i=1}^n g_i^2 \right)^2}{\sum_{i=1}^n \omega_i^2 g_i^4}.
\]

\subsection{Saddlepoint approximation}

\citet{McCaffrey2006improved} developed small-sample adjustments to test statistics based on cluster-robust variance estimators, of which HC variance estimators are a special case. They consider both a Satterthwaite approximation (similar to Lipsitz et al.) and a saddlepoint approximation for the distribution of the test statistic, finding that the latter produced tests with more accurate size. The saddlepoint approximation is obtained as follows. Let Observe that the cumulative distribution of $t^{HC}_q$ can be expressed as \[
\Pr\left(t^{HC}_q \leq t\right) = \Pr\left(\frac{\left(\hat\beta_q - c\right)^2}{\Var(\hat\beta_q)} - t^2 \frac{V^{HC}_{qq}}{\Var(\hat\beta_q)} \leq 0\right). \]
Note that $\left(\hat\beta_q - c\right)^2 / \Var(\hat\beta_q) ~ \chi^2_1$ and that $V^{HC}_{qq}$ is distributed as a weighted sum of $\chi^2_1$ random variables, with weights given by the eigen-values $\lambda_1,...,\lambda_n$ of the matrix $\bm{A}_{xq} \left(\bm{I} - \bm{H}\right)\bs\Sigma\left(\bm{I} - \bm{H}\right)$. Assuming that $V^{HC}_{qq}$ is unbiased, so that \[
\E\left(V^{HC}_{qq}\right) = \tr\left[\bm{A}_{xq} \left(\bm{I} - \bm{H}\right)\bs\Sigma\left(\bm{I} - \bm{H}\right)\right] = \sum_{j=1}^n \lambda_j, \]
and that $\hat\beta_q$ is independent of $V^{HC}_{qq}$, it follows that the $\Pr\left(t^{HC}_q \leq t\right)$ can be expressed as $\Pr(Z \leq 0)$, where $Z = \sum_{i=0}^n \gamma_i z_i$,
$\gamma_0 = 1$, $\gamma_i = -t^2 \lambda_i / \sum_{j=1}^n \lambda_j$, and $z_0,...,z_n \iid \chi^2_1$.

The saddlepoint technique is a means to approximate the distribution of $Z$. Let $s$ be the saddlepoint, defined implicitly as the solution to \[
\sum_{i=0}^n \frac{\gamma_i}{1 - 2 \gamma_i s} = 0. \]
Note that the solution to the saddlepoint equation will be in the range $\left(\left(2 \min\left\{\gamma_0,...,\gamma_n\right\}\right)^{-1}, 0 \right)$ if $\sum_{i=0}^n \gamma_i > 0$ and in the range $\left(0, \left(2 \max\left\{\gamma_0,...,\gamma_n\right\}\right)^{-1}\right)$ if $\sum_{i=0}^n \gamma_i \leq 0$. Define the quantities $r$ and $q$ as \[
r = \text{sign}(s)\sqrt{2sz + \sum_{i=0}^n \log\left(1 - 2\gamma_i s\right)}, \qquad q = s \sqrt{2 \sum_{i=0}^n \frac{\gamma_i^2}{\left(1 - 2 \gamma_i s\right)^2}} \]
for a constant $z$. The saddlepoint approximation is then 
\begin{equation}
\label{eq:saddlepoint_approx}
\Pr(Z \leq z) \approx \begin{cases}\Phi(r) + \phi(r)\left[\frac{1}{r} - \frac{1}{q}\right] & s \neq 0 \\
\frac{1}{2} + \frac{\sum_{i=0}^n \gamma_i^3}{3 \sqrt\pi \left(\sum_{i=0}^n \gamma_i^2\right)^{3/2}} & s = 0. \end{cases}
\end{equation}
Given an observed value for the $t$-statistic $t^{HC}_q$, a $p$-value for $H_0$ can be calculated by taking $\gamma_i = -\left(t^{HC}_q\right)^2 \lambda_i / \sum_{j=1}^n \lambda_j$ for $i = 1,...,n$, finding $s$, $r$, and $q$, and evaluating $1 - \Pr(Z \leq 0)$ using Equation (\ref{eq:saddlepoint_approx}).

\section{Simulation study}
\label{sec:verify}

\section{Conclusion}
\label{sec:conc}


\newpage
\begin{center}
{\large\bf SUPPLEMENTARY MATERIAL}
\end{center}

\begin{description}

\item[Title:] Brief description. (file type)

\item[R-package for  MYNEW routine:] R-package ?MYNEW? containing code to perform the diagnostic methods described in the article. The package also contains all datasets used as examples in the article. (GNU zipped tar file)

\item[HIV data set:] Data set used in the illustration of MYNEW method in Section~ 3.2. (.txt file)

\end{description}

\bibliographystyle{agsm}
\bibliography{Bibliography}

\end{document}
